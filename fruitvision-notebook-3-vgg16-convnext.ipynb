{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12513106,"sourceType":"datasetVersion","datasetId":7898069},{"sourceId":12506883,"sourceType":"datasetVersion","datasetId":7893794}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing necessary Libraries and Checking the Device specifications\n","metadata":{}},{"cell_type":"code","source":"# --- Core Libraries ---\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms, models\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom torch.cuda.amp import GradScaler, autocast\n\n# --- Analytics & Visualization ---\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport random\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-13T02:52:03.192Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GPU Setup and Verification\n","metadata":{}},{"cell_type":"code","source":"def setup_gpu():\n    if torch.cuda.is_available():\n        device_count = torch.cuda.device_count()\n        current_device = torch.cuda.current_device()\n        device_name = torch.cuda.get_device_name(current_device)\n        \n        print(\"=\"*60)\n        print(\"TRANSFER LEARNING 2 - GPU CONFIGURATION\")\n        print(\"=\"*60)\n        print(f\"✅ CUDA Available: {torch.cuda.is_available()}\")\n        print(f\"✅ PyTorch Version: {torch.__version__}\")\n        print(f\"✅ GPU Count: {device_count}\")\n        print(f\"✅ Current GPU: {current_device}\")\n        print(f\"✅ GPU Name: {device_name}\")\n        \n        gpu_memory = torch.cuda.get_device_properties(current_device).total_memory / 1e9\n        print(f\"✅ GPU Memory: {gpu_memory:.2f} GB\")\n        \n        cudnn.benchmark = True\n        cudnn.deterministic = True\n        \n        return f'cuda:{current_device}'\n    else:\n        print(\"❌ CUDA not available!\")\n        return 'cpu'\n\nDEVICE = setup_gpu()\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-13T02:52:03.194Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Importing the dataset","metadata":{}},{"cell_type":"code","source":"# --- Reproducibility ---\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\n\n# --- Configuration ---\nDATA_DIR_VARIETY = '/kaggle/input/fruitvision-a-benchmark-dataset-for-fresh/Fruits Original'\n\n\nBATCH_SIZE = 24  # Reduced for VGG16 which uses more memory\nIMG_SIZE = (224, 224)\nLEARNING_RATE = 0.0001\nEPOCHS = 20\nprint(f\"✅ Configuration: Batch={BATCH_SIZE}, Epochs={EPOCHS}, LR={LEARNING_RATE}\")\n\n# --- GPU Memory Management ---\ndef clear_gpu_memory():\n    if DEVICE.startswith('cuda'):\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats(DEVICE)\n\ndef print_gpu_memory():\n    if DEVICE.startswith('cuda'):\n        allocated = torch.cuda.memory_allocated(DEVICE) / 1e9\n        cached = torch.cuda.memory_reserved(DEVICE) / 1e9\n        max_alloc = torch.cuda.max_memory_allocated(DEVICE) / 1e9\n        print(f\"GPU Memory - Allocated: {allocated:.2f}GB, Cached: {cached:.2f}GB, Max: {max_alloc:.2f}GB\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-13T02:52:03.195Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Transformers and DataLoaders ","metadata":{}},{"cell_type":"code","source":"train_transforms = transforms.Compose([\n    transforms.TrivialAugmentWide(),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=10),\n    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n    transforms.Resize(IMG_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_test_transforms = transforms.Compose([\n    transforms.Resize(IMG_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# --- GPU-Optimized DataLoader ---\ndef create_dataloaders(dataset_path, batch_size=24):\n    full_dataset = datasets.ImageFolder(dataset_path)\n    \n    total_size = len(full_dataset)\n    train_size = int(0.7 * total_size)\n    val_size = int(0.15 * total_size)\n    test_size = total_size - train_size - val_size\n    \n    train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n    \n    train_dataset.dataset.transform = train_transforms\n    val_dataset.dataset.transform = val_test_transforms\n    test_dataset.dataset.transform = val_test_transforms\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                             num_workers=4, pin_memory=True, persistent_workers=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n                           num_workers=4, pin_memory=True, persistent_workers=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n                            num_workers=4, pin_memory=True, persistent_workers=True)\n    \n    return train_loader, val_loader, test_loader, train_size, val_size, test_size\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-13T02:52:03.195Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transfer Learning Model Function","metadata":{}},{"cell_type":"code","source":"# --- Transfer Learning Model Creation ---\ndef create_transfer_model(model_name, num_classes, freeze_backbone=True):\n    print(f\"\\n--- Creating {model_name} ---\")\n    \n    if model_name == \"vgg16\":\n        model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n        \n        if freeze_backbone:\n            for param in model.features.parameters():\n                param.requires_grad = False\n            print(f\"✅ Backbone frozen for {model_name}\")\n        \n        # Replace classifier with more efficient one\n        model.classifier = nn.Sequential(\n            nn.Linear(25088, 4096),\n            nn.ReLU(True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 1024),\n            nn.ReLU(True),\n            nn.Dropout(0.5),\n            nn.Linear(1024, 512),\n            nn.ReLU(True),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n        \n    elif model_name == \"convnext_tiny\":\n        model = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n        \n        if freeze_backbone:\n            for param in model.features.parameters():\n                param.requires_grad = False\n            print(f\"✅ Backbone frozen for {model_name}\")\n        \n        # Replace classifier\n        model.classifier = nn.Sequential(\n            nn.Flatten(1),\n            nn.LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n            nn.Dropout(0.2),\n            nn.Linear(768, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(256, num_classes)\n        )\n    \n    else:\n        raise ValueError(f\"Model {model_name} not supported\")\n    \n    # Count parameters\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"✅ Total params: {total_params:,}\")\n    print(f\"✅ Trainable params: {trainable_params:,}\")\n    \n    return model.to(DEVICE)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-13T02:52:03.196Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Function Declaration","metadata":{}},{"cell_type":"code","source":"# --- Training Function ---\ndef train_transfer_model(model, model_name, train_loader, val_loader, epochs, lr, device, task_type):\n    print(f\"\\n--- Training {model_name} for {task_type} ---\")\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n                           lr=lr, weight_decay=1e-4)\n    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, eta_min=1e-7)\n    scaler = GradScaler()\n    \n    # Early stopping\n    early_stopping_patience = 7\n    min_val_loss = float('inf')\n    epochs_no_improve = 0\n    best_model_state = None\n    \n    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n    \n    clear_gpu_memory()\n    \n    for epoch in range(epochs):\n        # Training phase\n        model.train()\n        train_loss, train_correct, train_total = 0, 0, 0\n        \n        for batch_idx, (inputs, labels) in enumerate(train_loader):\n            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n            \n            optimizer.zero_grad()\n            \n            with autocast():\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n            \n            # Scale loss for mixed precision\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            train_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs.data, 1)\n            train_total += labels.size(0)\n            train_correct += (predicted == labels).sum().item()\n            \n            # Memory management for VGG16\n            if batch_idx % 50 == 0:\n                torch.cuda.empty_cache()\n        \n        # Validation phase\n        model.eval()\n        val_loss, val_correct, val_total = 0, 0, 0\n        \n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n                \n                with autocast():\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n                \n                val_loss += loss.item() * inputs.size(0)\n                _, predicted = torch.max(outputs.data, 1)\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n        \n        # Calculate metrics\n        avg_train_loss = train_loss / train_total\n        avg_val_loss = val_loss / val_total\n        avg_train_acc = train_correct / train_total\n        avg_val_acc = val_correct / val_total\n        \n        history['train_loss'].append(avg_train_loss)\n        history['val_loss'].append(avg_val_loss)\n        history['train_acc'].append(avg_train_acc)\n        history['val_acc'].append(avg_val_acc)\n        \n        # Print progress\n        current_lr = optimizer.param_groups[0]['lr']\n        print(f\"Epoch {epoch+1}/{epochs} | Train Acc: {avg_train_acc:.4f} | Val Acc: {avg_val_acc:.4f} | \"\n              f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {current_lr:.6f}\")\n        \n        scheduler.step()\n        \n        # Early stopping and model saving\n        if avg_val_loss < min_val_loss:\n            min_val_loss = avg_val_loss\n            epochs_no_improve = 0\n            best_model_state = model.state_dict().copy()\n            torch.save(model.state_dict(), f'{model_name}_{task_type.lower().replace(\" \", \"_\")}_best.pth')\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= early_stopping_patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n        \n        if epoch % 5 == 0:\n            print_gpu_memory()\n    \n    # Load best model\n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n    \n    clear_gpu_memory()\n    return history","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-13T02:52:03.196Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Evaluation and Metrics Defining Functions","metadata":{}},{"cell_type":"code","source":"# --- Evaluation Function ---\ndef evaluate_transfer_model(model, model_name, test_loader, class_names, device, task_type):\n    print(f\"\\n--- Evaluating {model_name} for {task_type} ---\")\n    model.eval()\n    y_pred, y_true = [], []\n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device, non_blocking=True)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            y_pred.extend(predicted.cpu().numpy())\n            y_true.extend(labels.cpu().numpy())\n    \n    # Calculate metrics\n    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True, zero_division=0)\n    accuracy = report['accuracy']\n    f1_macro = report['macro avg']['f1-score']\n    \n    print(f\"✅ Test Accuracy: {accuracy:.4f}\")\n    print(f\"✅ F1-Macro Score: {f1_macro:.4f}\")\n    \n    # Detailed report\n    print(\"\\nDetailed Classification Report:\")\n    print(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))\n    \n    # Confusion Matrix\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n                xticklabels=class_names, yticklabels=class_names)\n    plt.title(f'Confusion Matrix: {model_name} ({task_type})', fontsize=14)\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    plt.xticks(rotation=45)\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.show()\n    \n    return {'accuracy': accuracy, 'f1_macro': f1_macro}","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-13T02:52:03.197Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## VGG16 Training","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*60)\nprint(\"FRUITVISION\")\nprint(\"=\"*60)\n\n# Load class information\nclass_names_variety = sorted(os.listdir(DATA_DIR_VARIETY))\nNUM_CLASSES_VARIETY = len(class_names_variety)\nprint(f\"Variety Classes ({NUM_CLASSES_VARIETY}): {class_names_variety}\")\n\n# Create data loaders\ntrain_loader_v, val_loader_v, test_loader_v, train_size_v, val_size_v, test_size_v = create_dataloaders(\n    DATA_DIR_VARIETY, BATCH_SIZE\n)\nprint(f\"Dataset sizes: Train={train_size_v}, Val={val_size_v}, Test={test_size_v}\")\n\n# === VGG16 FOR VARIETY CLASSIFICATION ===\nprint(\"\\n\" + \"=\"*40)\nprint(\"VGG16 - VARIETY CLASSIFICATION\")\nprint(\"=\"*40)\n\nvgg16_variety = create_transfer_model(\"vgg16\", NUM_CLASSES_VARIETY, freeze_backbone=True)\nhistory_vgg_variety = train_transfer_model(vgg16_variety, \"VGG16\", \n                                          train_loader_v, val_loader_v, EPOCHS, \n                                          LEARNING_RATE, DEVICE, \"Variety Classification\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-13T02:52:03.197Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## VGG16 Evaluation","metadata":{}},{"cell_type":"code","source":"# Plot training curves\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history_vgg_variety['train_acc'], label='Train', marker='o')\nplt.plot(history_vgg_variety['val_acc'], label='Validation', marker='s')\nplt.title('VGG16 Variety - Accuracy', fontsize=14)\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.plot(history_vgg_variety['train_loss'], label='Train', marker='o')\nplt.plot(history_vgg_variety['val_loss'], label='Validation', marker='s')\nplt.title('VGG16 Variety - Loss', fontsize=14)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Evaluate VGG16\nresults_vgg_variety = evaluate_transfer_model(vgg16_variety, \"VGG16\", \n                                             test_loader_v, class_names_variety, \n                                             DEVICE, \"Variety Classification\")\n\n# Save model info\nmodel_info_vgg_variety = {\n    'model_name': 'VGG16_Variety',\n    'architecture': 'VGG16 (Transfer Learning)',\n    'num_classes': NUM_CLASSES_VARIETY,\n    'class_names': class_names_variety,\n    'input_size': IMG_SIZE,\n    'accuracy': results_vgg_variety['accuracy'],\n    'f1_macro': results_vgg_variety['f1_macro'],\n    'frozen_backbone': True,\n    'epochs_trained': len(history_vgg_variety['train_acc'])\n}\n\nwith open('vgg16_variety_info.pkl', 'wb') as f:\n    pickle.dump(model_info_vgg_variety, f)\n\nclear_gpu_memory()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-13T02:52:03.197Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ConvNeXt Training and evaluation","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*40)\nprint(\"CONVNEXT-TINY - VARIETY CLASSIFICATION\")\nprint(\"=\"*40)\n\nconvnext_variety = create_transfer_model(\"convnext_tiny\", NUM_CLASSES_VARIETY, freeze_backbone=True)\nhistory_convnext_variety = train_transfer_model(convnext_variety, \"ConvNeXt-Tiny\",\n                                               train_loader_v, val_loader_v, EPOCHS,\n                                               LEARNING_RATE, DEVICE, \"Variety Classification\")\n\n# Plot training curves\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history_convnext_variety['train_acc'], label='Train', marker='o')\nplt.plot(history_convnext_variety['val_acc'], label='Validation', marker='s')\nplt.title('ConvNeXt-Tiny Variety - Accuracy', fontsize=14)\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.plot(history_convnext_variety['train_loss'], label='Train', marker='o')\nplt.plot(history_convnext_variety['val_loss'], label='Validation', marker='s')\nplt.title('ConvNeXt-Tiny Variety - Loss', fontsize=14)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Evaluate ConvNeXt-Tiny\nresults_convnext_variety = evaluate_transfer_model(convnext_variety, \"ConvNeXt-Tiny\",\n                                                  test_loader_v, class_names_variety,\n                                                  DEVICE, \"Variety Classification\")\n\n# Save model info\nmodel_info_convnext_variety = {\n    'model_name': 'ConvNeXt-Tiny_Variety',\n    'architecture': 'ConvNeXt-Tiny (Transfer Learning)',\n    'num_classes': NUM_CLASSES_VARIETY,\n    'class_names': class_names_variety,\n    'input_size': IMG_SIZE,\n    'accuracy': results_convnext_variety['accuracy'],\n    'f1_macro': results_convnext_variety['f1_macro'],\n    'frozen_backbone': True,\n    'epochs_trained': len(history_convnext_variety['train_acc'])\n}\n\nwith open('convnext_tiny_variety_info.pkl', 'wb') as f:\n    pickle.dump(model_info_convnext_variety, f)\n\nclear_gpu_memory()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-13T02:52:03.197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === COMPARISON TABLE ===\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRANSFER LEARNING MODELS COMPARISON\")\nprint(\"=\"*60)\n\n# Create comparison dataframe\ncomparison_data = [\n    {\n        'Model': 'VGG16',\n        'Task': 'Variety Classification',\n        'Accuracy': results_vgg_variety['accuracy'],\n        'F1-Macro': results_vgg_variety['f1_macro'],\n        'Epochs': len(history_vgg_variety['train_acc'])\n    },\n    {\n        'Model': 'ConvNeXt-Tiny',\n        'Task': 'Variety Classification', \n        'Accuracy': results_convnext_variety['accuracy'],\n        'F1-Macro': results_convnext_variety['f1_macro'],\n        'Epochs': len(history_convnext_variety['train_acc'])\n    },\n    {\n        'Model': 'VGG16',\n        'Task': 'Ripeness Detection',\n        'Accuracy': results_vgg_ripeness['accuracy'],\n        'F1-Macro': results_vgg_ripeness['f1_macro'],\n        'Epochs': len(history_vgg_ripeness['train_acc'])\n    },\n    {\n        'Model': 'ConvNeXt-Tiny',\n        'Task': 'Ripeness Detection',\n        'Accuracy': results_convnext_ripeness['accuracy'],\n        'F1-Macro': results_convnext_ripeness['f1_macro'],\n        'Epochs': len(history_convnext_ripeness['train_acc'])\n    }\n]\n\ncomparison_df = pd.DataFrame(comparison_data)\nprint(comparison_df.to_string(index=False, float_format='%.4f'))\n\n# Visualization of results\nplt.figure(figsize=(15, 6))\n\nplt.subplot(1, 2, 1)\nvariety_models = ['VGG16', 'ConvNeXt-Tiny']\nvariety_acc = [results_vgg_variety['accuracy'], results_convnext_variety['accuracy']]\nplt.bar(variety_models, variety_acc, color=['purple', 'gold'])\nplt.title('Variety Classification - Test Accuracy')\nplt.ylabel('Accuracy')\nplt.ylim(0, 1)\nfor i, v in enumerate(variety_acc):\n    plt.text(i, v + 0.01, f'{v:.3f}', ha='center')\n\nplt.subplot(1, 2, 2)\nripeness_models = ['VGG16', 'ConvNeXt-Tiny']\nripeness_acc = [results_vgg_ripeness['accuracy'], results_convnext_ripeness['accuracy']]\nplt.bar(ripeness_models, ripeness_acc, color=['darkgreen', 'crimson'])\nplt.title('Ripeness Detection - Test Accuracy')\nplt.ylabel('Accuracy')\nplt.ylim(0, 1)\nfor i, v in enumerate(ripeness_acc):\n    plt.text(i, v + 0.01, f'{v:.3f}', ha='center')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n✅ All transfer learning models 2 trained and evaluated successfully!\")\nprint(\"✅ Model weights and metadata saved\")\nprint(\"✅ GPU memory managed efficiently for VGG16\")\n\nclear_gpu_memory()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-13T02:52:03.197Z"}},"outputs":[],"execution_count":null}]}